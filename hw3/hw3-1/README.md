# Model

```
InfoGAN(
  (G): Generator(
    (linear): Sequential(
      (0): Linear(in_features=133, out_features=32768, bias=True)
    )
    (seq): Sequential(
      (0): BatchNorm2d(32768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU()
      (2): ConvTranspose2d(32768, 128, kernel_size=(4, 4), stride=(1, 1))
      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): ReLU()
      (5): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(3, 3))
      (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): ReLU()
      (8): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(5, 5))
      (9): Tanh()
    )
  )
  (D): Discriminator(
    (linear1): Linear(in_features=1384448, out_features=512, bias=True)
    (linear2): Linear(in_features=512, out_features=1, bias=True)
    (reconstr): Linear(in_features=512, out_features=5, bias=True)
    (sig): Sigmoid()
    (seq): Sequential(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))
      (3): LeakyReLU(negative_slope=0.1)
      (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))
      (5): LeakyReLU(negative_slope=0.1)
      (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))
      (7): LeakyReLU(negative_slope=0.1)
    )
  )
)
```

## Initialization

```Model(z_dim, c_dim)```

The model requires two parameters: dimension of noise (```z_dim```) and dimension of latent code (```c_dim```).

## Parameters

```Model.parameters()```

The method ```parameters()``` returns a dictionary with three keys: ```'generator'``` corresponds to the parameters of the generator, ```'discriminator'``` to those of the discriminator, and ```'all'``` to all parameters.

## Forward

```Model.forward(x, z=None, c=None)```

Depending on the state of the model, there are two mechanisms of forward. Both returns a dictionary of tensors. The model state can be toggled by methods ```Model.train_discriminator()``` and ```Model.train_generator()```.

1) training discriminator: the generator produces a batch of fake data depending on ```z``` and ```c``` (both of which can be specified by user). Both the batch of true data ```x``` and that of false data is passed through the discriminator. The returned tensors are:

```'raw_true'```: raw output of true data  
```'raw_false'```: raw output of false data  
```'score_true'```: sigmoid score of true data  
```'score_false'```: sigmoid score of false data  
```'false_data'```: false data generated by the generator  
```'latent_code'```: latent code fed into the generator  
```'reconstructed_code'```: reconstructed latent code  

2) training generator: the generator produces a batch of fake data depending on ```z``` and ```c``` (both of which can be specified by user). This batch is then passed through the discriminator. The returned tensors are:

```'raw_false'```: raw output of false data  
```'score_false'```: sigmoid score of false data  
```'noise'```: input of generator (concatenation of ```z``` and ```c```)  
